{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8a2e1765",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "37eb9261",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'D-1-20.xlsx'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# Load trace_D-1-19_trimmed.xlsx separately\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m df1 = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_excel\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mD-1-20.xlsx\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      7\u001b[39m df2 = pd.read_excel(\u001b[33m\"\u001b[39m\u001b[33mD-1-1A.xlsx\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      8\u001b[39m df3 = pd.read_excel(\u001b[33m\"\u001b[39m\u001b[33mD-1-2.xlsx\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mg:\\deep_learning_inversion\\augmented_traces_on_impedance(seismic to impedance)\\venv\\Lib\\site-packages\\pandas\\io\\excel\\_base.py:495\u001b[39m, in \u001b[36mread_excel\u001b[39m\u001b[34m(io, sheet_name, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, parse_dates, date_parser, date_format, thousands, decimal, comment, skipfooter, storage_options, dtype_backend, engine_kwargs)\u001b[39m\n\u001b[32m    493\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(io, ExcelFile):\n\u001b[32m    494\u001b[39m     should_close = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m495\u001b[39m     io = \u001b[43mExcelFile\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    496\u001b[39m \u001b[43m        \u001b[49m\u001b[43mio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    497\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    498\u001b[39m \u001b[43m        \u001b[49m\u001b[43mengine\u001b[49m\u001b[43m=\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    499\u001b[39m \u001b[43m        \u001b[49m\u001b[43mengine_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mengine_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    500\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    501\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m engine \u001b[38;5;129;01mand\u001b[39;00m engine != io.engine:\n\u001b[32m    502\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    503\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mEngine should not be specified when passing \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    504\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33man ExcelFile - ExcelFile already has the engine set\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    505\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mg:\\deep_learning_inversion\\augmented_traces_on_impedance(seismic to impedance)\\venv\\Lib\\site-packages\\pandas\\io\\excel\\_base.py:1550\u001b[39m, in \u001b[36mExcelFile.__init__\u001b[39m\u001b[34m(self, path_or_buffer, engine, storage_options, engine_kwargs)\u001b[39m\n\u001b[32m   1548\u001b[39m     ext = \u001b[33m\"\u001b[39m\u001b[33mxls\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1549\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1550\u001b[39m     ext = \u001b[43minspect_excel_format\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1551\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcontent_or_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\n\u001b[32m   1552\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1553\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m ext \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1554\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1555\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mExcel file format cannot be determined, you must specify \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1556\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33man engine manually.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1557\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mg:\\deep_learning_inversion\\augmented_traces_on_impedance(seismic to impedance)\\venv\\Lib\\site-packages\\pandas\\io\\excel\\_base.py:1402\u001b[39m, in \u001b[36minspect_excel_format\u001b[39m\u001b[34m(content_or_path, storage_options)\u001b[39m\n\u001b[32m   1399\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(content_or_path, \u001b[38;5;28mbytes\u001b[39m):\n\u001b[32m   1400\u001b[39m     content_or_path = BytesIO(content_or_path)\n\u001b[32m-> \u001b[39m\u001b[32m1402\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1403\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcontent_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrb\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[32m   1404\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m handle:\n\u001b[32m   1405\u001b[39m     stream = handle.handle\n\u001b[32m   1406\u001b[39m     stream.seek(\u001b[32m0\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mg:\\deep_learning_inversion\\augmented_traces_on_impedance(seismic to impedance)\\venv\\Lib\\site-packages\\pandas\\io\\common.py:882\u001b[39m, in \u001b[36mget_handle\u001b[39m\u001b[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[39m\n\u001b[32m    873\u001b[39m         handle = \u001b[38;5;28mopen\u001b[39m(\n\u001b[32m    874\u001b[39m             handle,\n\u001b[32m    875\u001b[39m             ioargs.mode,\n\u001b[32m   (...)\u001b[39m\u001b[32m    878\u001b[39m             newline=\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    879\u001b[39m         )\n\u001b[32m    880\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    881\u001b[39m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m882\u001b[39m         handle = \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    883\u001b[39m     handles.append(handle)\n\u001b[32m    885\u001b[39m \u001b[38;5;66;03m# Convert BytesIO or file objects passed with an encoding\u001b[39;00m\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'D-1-20.xlsx'"
     ]
    }
   ],
   "source": [
    "# ---- Extra loading part for augmented traces ----\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Load trace_D-1-19_trimmed.xlsx separately\n",
    "df1 = pd.read_excel(\"D-1-20.xlsx\")\n",
    "df2 = pd.read_excel(\"D-1-1A.xlsx\")\n",
    "df3 = pd.read_excel(\"D-1-2.xlsx\")\n",
    "df4 = pd.read_excel(\"D-1-4.xlsx\")\n",
    "df5 = pd.read_excel(\"D-1-12.xlsx\")\n",
    "df6 = pd.read_excel(\"D-1-13.xlsx\")\n",
    "df7 = pd.read_excel(\"D-1-15.xlsx\")\n",
    "df8 = pd.read_excel(\"D-1-19.xlsx\")\n",
    "df9 = pd.read_excel(\"D-1A-1.xlsx\")\n",
    "df10 = pd.read_excel(\"D1-C1.xlsx\")\n",
    "df11 = pd.read_excel(\"D1-D1.xlsx\")\n",
    "df12 = pd.read_excel(\"noisy_0.01_D-1-1A.xlsx\")\n",
    "df13 = pd.read_excel(\"noisy_0.01_D-1-2.xlsx\")\n",
    "df14 = pd.read_excel(\"noisy_0.01_D-1-4.xlsx\")\n",
    "df15 = pd.read_excel(\"noisy_0.01_D-1-12.xlsx\")\n",
    "# df16 = pd.read_excel(\"noisy_0.01_D-1-15.xlsx\")\n",
    "#df17 = pd.read_excel(\"noisy_0.01_D-1-19.xlsx\")\n",
    "df18 = pd.read_excel(\"noisy_0.01_D-1A-1.xlsx\")\n",
    "df19 = pd.read_excel(\"noisy_0.01_D1-C1.xlsx\")\n",
    "df20 = pd.read_excel(\"noisy_0.01_D1-D1.xlsx\")\n",
    "df21 = pd.read_excel(\"noisy_0.005_D-1-1A.xlsx\")\n",
    "df22 = pd.read_excel(\"noisy_0.005_D-1-2.xlsx\")\n",
    "df23 = pd.read_excel(\"noisy_0.005_D-1-4.xlsx\")\n",
    "df24 = pd.read_excel(\"noisy_0.005_D-1-12.xlsx\")\n",
    "# df25 = pd.read_excel(\"noisy_0.005_D-1-15.xlsx\")\n",
    "#df26 = pd.read_excel(\"noisy_0.005_D-1-19.xlsx\")\n",
    "df27 = pd.read_excel(\"noisy_0.005_D-1A-1.xlsx\")\n",
    "df28 = pd.read_excel(\"noisy_0.005_D1-C1.xlsx\")\n",
    "df29 = pd.read_excel(\"noisy_0.005_D1-D1.xlsx\")\n",
    "df30 = pd.read_excel(\"noisy_0.008_D-1-1A.xlsx\")\n",
    "df31 = pd.read_excel(\"noisy_0.008_D-1-2.xlsx\")\n",
    "df32 = pd.read_excel(\"noisy_0.008_D-1-4.xlsx\")\n",
    "df33 = pd.read_excel(\"noisy_0.008_D-1-12.xlsx\")\n",
    "# df34 = pd.read_excel(\"noisy_0.008_D-1-15.xlsx\")\n",
    "#df35 = pd.read_excel(\"noisy_0.008_D-1-19.xlsx\")\n",
    "df36 = pd.read_excel(\"noisy_0.008_D-1A-1.xlsx\")\n",
    "df37 = pd.read_excel(\"noisy_0.008_D1-C1.xlsx\")\n",
    "df38 = pd.read_excel(\"noisy_0.008_D1-D1.xlsx\")\n",
    "\n",
    "\n",
    "# -----------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df589f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# File paths\n",
    "file_paths = [\n",
    "    \"D-1-20.xlsx\", \"D-1-1A.xlsx\", \"D-1-2.xlsx\", \"D-1-4.xlsx\",\n",
    "    \"D-1-12.xlsx\", \"D-1-13.xlsx\", \"D-1-15.xlsx\", \"D-1-19.xlsx\", \"D-1A-1.xlsx\",\n",
    "    \"D1-C1.xlsx\", \"D1-D1.xlsx\",\n",
    "    \"noisy_0.01_D-1-1A.xlsx\", \"noisy_0.01_D-1-2.xlsx\", \"noisy_0.01_D-1-4.xlsx\",\n",
    "    \"noisy_0.01_D-1-12.xlsx\", #\"noisy_0.01_D-1-15.xlsx\", \n",
    "    \"noisy_0.01_D-1-19.xlsx\", \"noisy_0.01_D-1A-1.xlsx\",\n",
    "    \"noisy_0.01_D1-C1.xlsx\", \"noisy_0.01_D1-D1.xlsx\",\n",
    "    \"noisy_0.005_D-1-1A.xlsx\", \"noisy_0.005_D-1-2.xlsx\", \"noisy_0.005_D-1-4.xlsx\",\n",
    "    \"noisy_0.005_D-1-12.xlsx\", #\"noisy_0.005_D-1-15.xlsx\", \n",
    "    \"noisy_0.005_D-1-19.xlsx\", \"noisy_0.005_D-1A-1.xlsx\",\n",
    "    \"noisy_0.005_D1-C1.xlsx\", \"noisy_0.005_D1-D1.xlsx\",\n",
    "    \"noisy_0.008_D-1-1A.xlsx\", \"noisy_0.008_D-1-2.xlsx\", \"noisy_0.008_D-1-4.xlsx\",\n",
    "    \"noisy_0.008_D-1-12.xlsx\", #\"noisy_0.008_D-1-15.xlsx\", \n",
    "    \"noisy_0.008_D-1-19.xlsx\", \"noisy_0.008_D-1A-1.xlsx\",\n",
    "    \"noisy_0.008_D1-C1.xlsx\", \"noisy_0.008_D1-D1.xlsx\"\n",
    "]\n",
    "\n",
    "labels = [\n",
    "    \"D-1-20\", \"D-1-1A\", \"D-1-2\", \"D-1-4\",\n",
    "    \"D-1-12\", \"D-1-13\", \"D-1-15\", \"D-1-19\", \"D-1A-1\",\n",
    "    \"D1-C1\", \"D1-D1\",\n",
    "    \"noisy_0.01_D-1-1A\", \"noisy_0.01_D-1-2\", \"noisy_0.01_D-1-4\",\n",
    "    \"noisy_0.01_D-1-12\", #\"noisy_0.01_D-1-15\", \n",
    "    \"noisy_0.01_D-1-19\", \"noisy_0.01_D-1A-1\",\n",
    "    \"noisy_0.01_D1-C1\", \"noisy_0.01_D1-D1\",\n",
    "    \"noisy_0.005_D-1-1A\", \"noisy_0.005_D-1-2\", \"noisy_0.005_D-1-4\",\n",
    "    \"noisy_0.005_D-1-12\", #\"noisy_0.005_D-1-15\", \n",
    "    \"noisy_0.005_D-1-19\", \"noisy_0.005_D-1A-1\",\n",
    "    \"noisy_0.005_D1-C1\", \"noisy_0.005_D1-D1\",\n",
    "    \"noisy_0.008_D-1-1A\", \"noisy_0.008_D-1-2\", \"noisy_0.008_D-1-4\",\n",
    "    \"noisy_0.008_D-1-12\", #\"noisy_0.008_D-1-15\", \n",
    "    \"noisy_0.008_D-1-19\", \"noisy_0.008_D-1A-1\",\n",
    "    \"noisy_0.008_D1-C1\", \"noisy_0.008_D1-D1\"\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "# Create output directory\n",
    "os.makedirs(\"data\", exist_ok=True)\n",
    "\n",
    "# Containers\n",
    "seismic_traces = []\n",
    "impedance_traces = []\n",
    "\n",
    "# Load data\n",
    "for path in file_paths:\n",
    "    df = pd.read_excel(path)\n",
    "\n",
    "    # Read seismic and impedance columns\n",
    "    seismic = df[\"Impedance\"].iloc[:940].values  # First 940 from seismic\n",
    "    impedance = df[\"PhiE\"].iloc[:940].values        # First 940 from impedance\n",
    "\n",
    "    seismic_traces.append(seismic)\n",
    "    impedance_traces.append(impedance)\n",
    "\n",
    "# Convert to 2D arrays\n",
    "seismic_array = np.stack(seismic_traces)           # Shape: (12, 940)\n",
    "impedance_array = np.stack(impedance_traces)       # Shape: (12, 940)\n",
    "\n",
    "# Save as dictionary\n",
    "data_dic = {\n",
    "    \"seismic\": seismic_array,\n",
    "    \"acoustic_impedance\": impedance_array\n",
    "}\n",
    "np.save(\"data/data.npy\", data_dic)\n",
    "print(\"✅ Data saved to data/data.npy as 2D arrays\")\n",
    "print(f\"Seismic shape: {seismic_array.shape}, Impedance shape: {impedance_array.shape}\")\n",
    "\n",
    "# ----------------- Plotting -----------------\n",
    "\n",
    "n = seismic_array.shape[0]\n",
    "fig, axes = plt.subplots(nrows=n, ncols=2, figsize=(12, 2.5 * n))\n",
    "fig.suptitle(\"Seismic & Impedance Traces (NaNs Preserved)\", fontsize=16)\n",
    "\n",
    "for i in range(n):\n",
    "    seismic = seismic_array[i]\n",
    "    impedance = impedance_array[i]\n",
    "\n",
    "    t_seismic = np.arange(len(seismic))\n",
    "    t_impedance = np.arange(len(impedance))\n",
    "\n",
    "    # Plot seismic\n",
    "    axes[i, 0].plot(seismic, t_seismic, color='black')\n",
    "    axes[i, 0].invert_yaxis()\n",
    "    axes[i, 0].set_ylabel(\"Samples\")\n",
    "    axes[i, 0].set_title(f\"{labels[i]} - Seismic\")\n",
    "    axes[i, 0].grid(True)\n",
    "\n",
    "    # Plot impedance\n",
    "    axes[i, 1].plot(impedance, t_impedance, color='green')\n",
    "    axes[i, 1].invert_yaxis()\n",
    "    axes[i, 1].set_title(f\"{labels[i]} - Impedance\")\n",
    "    axes[i, 1].grid(True)\n",
    "\n",
    "# Label bottom x-axes\n",
    "axes[-1, 0].set_xlabel(\"Amplitude\")\n",
    "axes[-1, 1].set_xlabel(\"Impedance\")\n",
    "\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.97])\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "069cbbc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_dic = np.load(\"data/data.npy\", allow_pickle=True).item()\n",
    "seismic_data = data_dic[\"seismic\"]\n",
    "acoustic_impedance_data = data_dic[\"acoustic_impedance\"]\n",
    "\n",
    "# The index of \"D-1-19.xlsx\" in your original list\n",
    "index= 0  # 0-based index\n",
    "\n",
    "# Get the seismic and impedance trace\n",
    "seismic_trace = seismic_data[index]\n",
    "impedance_trace = acoustic_impedance_data[index]\n",
    "\n",
    "# Create sample axes\n",
    "time_seismic = np.arange(seismic_trace.shape[0])\n",
    "time_impedance = np.arange(impedance_trace.shape[0])\n",
    "\n",
    "# Plotting\n",
    "fig, axs = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "axs[0].plot(seismic_trace, time_seismic, color='black')\n",
    "axs[0].invert_yaxis()\n",
    "axs[0].set_title('Seismic Trace (D-1-19)')\n",
    "axs[0].set_xlabel('Amplitude')\n",
    "axs[0].set_ylabel('Time Sample')\n",
    "\n",
    "axs[1].plot(impedance_trace, time_impedance, color='blue')\n",
    "axs[1].invert_yaxis()\n",
    "axs[1].set_title('Impedance Trace (D-1-19)')\n",
    "axs[1].set_xlabel('Impedance')\n",
    "axs[1].set_ylabel('Time Sample')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a8e6e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# #%% Normalization\n",
    "class Normalization:\n",
    "    def __init__(self, mean_val=None, std_val=None):\n",
    "        self.mean_val = mean_val\n",
    "        self.std_val = std_val\n",
    "\n",
    "    def fit(self, x):\n",
    "        self.mean_val = np.mean(x)\n",
    "        self.std_val = np.std(x)\n",
    "        # Prevent divide-by-zero\n",
    "        if self.std_val == 0:\n",
    "            self.std_val = 1.0\n",
    "\n",
    "    def normalize(self, x):\n",
    "        if isinstance(x, list):  # List of tensors\n",
    "            return [(trace - self.mean_val) / self.std_val for trace in x]\n",
    "        else:  # Single tensor or array\n",
    "            return (x - self.mean_val) / self.std_val\n",
    "\n",
    "    def unnormalize(self, x):\n",
    "        if isinstance(x, list):  # List of tensors\n",
    "            return [trace * self.std_val + self.mean_val for trace in x]\n",
    "        else:  # Single tensor or array\n",
    "            return x * self.std_val + self.mean_val\n",
    "\n",
    "        \n",
    "\n",
    "# # #%% metrics\n",
    "# def metrics(pred, target):\n",
    "#     pred = pred.view(pred.size(0), -1)\n",
    "#     target = target.view(target.size(0), -1)\n",
    "\n",
    "#     mean_target = target.mean(dim=1, keepdim=True)\n",
    "#     std_target = target.std(dim=1, keepdim=True)\n",
    "#     std_pred = pred.std(dim=1, keepdim=True)\n",
    "\n",
    "#     correlation = ((pred - pred.mean(dim=1, keepdim=True)) * (target - mean_target)).mean(dim=1)\n",
    "#     correlation /= (std_pred * std_target).squeeze()\n",
    "#     correlation = correlation.mean().item()\n",
    "\n",
    "#     ss_res = ((target - pred) ** 2).sum(dim=1)\n",
    "#     ss_tot = ((target - mean_target) ** 2).sum(dim=1)\n",
    "#     r2 = 1 - ss_res / ss_tot\n",
    "#     r2 = r2.mean().item()\n",
    "\n",
    "#     return correlation, r2\n",
    "\n",
    "#%% Metrics\n",
    "def metrics(y,x):\n",
    "    #x: reference signal\n",
    "    #y: estimated signal\n",
    "    if torch.is_tensor(x):\n",
    "        if x.is_cuda:\n",
    "            x = x.cpu()\n",
    "        x = x.numpy()\n",
    "    if torch.is_tensor(y):\n",
    "        if y.is_cuda:\n",
    "            y = y.cpu()\n",
    "        y = y.numpy()\n",
    "\n",
    "    #corrlation\n",
    "    x_mean = np.mean(x, axis=-1, keepdims=True)\n",
    "    y_mean = np.mean(y, axis=-1, keepdims=True)\n",
    "    x_std = np.std(x, axis=-1, keepdims=True)\n",
    "    y_std = np.std(y, axis=-1, keepdims=True)\n",
    "    corr = np.mean((x-x_mean)*(y-y_mean), axis=-1,keepdims=True)/(x_std*y_std)\n",
    "\n",
    "    #coefficeint of determination (r2)\n",
    "    S_tot = np.sum((x-x_mean)**2, axis=-1, keepdims=True)\n",
    "    S_res = np.sum((x - y)**2, axis=-1, keepdims=True)\n",
    "\n",
    "    r2 = (1-S_res/S_tot)\n",
    "\n",
    "    return torch.tensor(corr), torch.tensor(r2)\n",
    "\n",
    "\n",
    "# def display_results(loss, property_corr, property_r2, args, header):\n",
    "#     property_corr = torch.mean(torch.tensor(property_corr)).item()\n",
    "#     property_r2 = torch.mean(torch.tensor(property_r2)).item()\n",
    "#     loss = torch.mean(torch.tensor(loss))\n",
    "#     print(\"loss: {:.4f}\\nCorrelation: {:0.4f}\\nr2 Coeff.  : {:0.4f}\".format(loss,property_corr,property_r2))\n",
    "\n",
    "\n",
    "def display_results(loss, property_corr, property_r2, seismic_corr, seismic_r2, args, header):\n",
    "    property_corr = torch.mean(torch.cat(property_corr), dim=0).squeeze()\n",
    "    property_r2 = torch.mean(torch.cat(property_r2), dim=0).squeeze()\n",
    "    seismic_corr = torch.mean(torch.cat(seismic_corr), dim=0).squeeze()\n",
    "    seismic_r2 = torch.mean(torch.cat(seismic_r2), dim=0).squeeze()\n",
    "    loss = torch.mean(torch.tensor(loss))\n",
    "    print(\"loss: {:.4f}\\nCorrelation: {:0.4f}\\nr2 Coeff.  : {:0.4f}\\nSeismic_Correlation: {:0.4f}\\nSeismic_r2 Coeff.: {:0.4f}\".format(loss,property_corr,property_r2,seismic_corr, seismic_r2))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6b5f671",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dic = np.load(\"data/data.npy\", allow_pickle=True).item()\n",
    "seismic_data = data_dic[\"seismic\"]\n",
    "acoustic_impedance_data = data_dic[\"acoustic_impedance\"]\n",
    "\n",
    "\n",
    "\n",
    "seismic_mean = torch.tensor(np.mean(seismic_data,keepdims=True)).float()\n",
    "seismic_std = torch.tensor(np.std(seismic_data,keepdims=True)).float()\n",
    "\n",
    "acoustic_mean= torch.tensor(np.mean(acoustic_impedance_data, keepdims=True)).float()\n",
    "acoustic_std = torch.tensor(np.std(acoustic_impedance_data,keepdims=True)).float()\n",
    "\n",
    "\n",
    "seismic_data = torch.tensor(seismic_data).float()\n",
    "acoustic_impedance_data = torch.tensor(acoustic_impedance_data).float()\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    seismic_data = seismic_data.cuda()\n",
    "    acoustic_impedance_data = acoustic_impedance_data.cuda()\n",
    "\n",
    "    seismic_mean = seismic_mean.cuda()\n",
    "    seismic_std = seismic_std.cuda()\n",
    "\n",
    "    acoustic_mean = acoustic_mean.cuda()\n",
    "    acoustic_std = acoustic_std.cuda()\n",
    "\n",
    "seismic_normalization = Normalization(mean_val=seismic_mean,\n",
    "                                          std_val=seismic_std)\n",
    "\n",
    "acoustic_normalization = Normalization(mean_val=acoustic_mean,\n",
    "                                          std_val=acoustic_std)\n",
    "\n",
    "\n",
    "seismic_data = seismic_normalization.normalize(seismic_data)\n",
    "acoustic_impedance_data = acoustic_normalization.normalize(acoustic_impedance_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bee1347e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select trace (e.g., 7th trace)\n",
    "index= 0\n",
    "seismic_norm = seismic_data[index]\n",
    "impedance_norm = acoustic_impedance_data[index]\n",
    "\n",
    "# Convert to numpy\n",
    "seismic_norm = seismic_norm.detach().cpu().numpy()\n",
    "impedance_norm = impedance_norm.detach().cpu().numpy()\n",
    "\n",
    "# Create sample axes\n",
    "time_seismic = np.arange(seismic_norm.shape[0])\n",
    "time_impedance = np.arange(impedance_norm.shape[0])\n",
    "\n",
    "# Plotting\n",
    "fig, axs = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "axs[0].plot(seismic_norm, time_seismic, color='black')\n",
    "axs[0].invert_yaxis()\n",
    "axs[0].set_title('Seismic Trace (D-1-19)')\n",
    "axs[0].set_xlabel('Amplitude')\n",
    "axs[0].set_ylabel('Time Sample')\n",
    "\n",
    "axs[1].plot(impedance_norm, time_impedance, color='blue')\n",
    "axs[1].invert_yaxis()\n",
    "axs[1].set_title('Impedance Trace (D-1-19)')\n",
    "axs[1].set_xlabel('Impedance')\n",
    "axs[1].set_ylabel('Time Sample')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf553fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Subset, TensorDataset\n",
    "\n",
    "def prepare_data_loaders(seismic_data, acoustic_impedance_data, \n",
    "                         test_index=0, batch_size=20, test=False):\n",
    "    \"\"\"\n",
    "    Prepare DataLoaders for training, validation, or testing.\n",
    "\n",
    "    Args:\n",
    "        seismic_data (torch.Tensor): Normalized seismic data [N, 1, 940]\n",
    "        acoustic_impedance_data (torch.Tensor): Normalized impedance data [N, 1, 940]\n",
    "        test_index (int): Index of the trace to use for testing\n",
    "        batch_size (int): Batch size for the DataLoader\n",
    "        test (bool): Whether to return test data loader\n",
    "\n",
    "    Returns:\n",
    "        For training mode:\n",
    "            train_loader, val_loader, unlabeled_loader\n",
    "        For test mode:\n",
    "            test_loader\n",
    "    \"\"\"\n",
    "    num_samples = seismic_data.shape[0]\n",
    "    all_indices = list(range(num_samples))\n",
    "\n",
    "    if not test:\n",
    "        # Fixed validation indices\n",
    "        val_indices = [6,7]\n",
    "\n",
    "        # Ensure they are within range\n",
    "        val_indices = [i for i in val_indices if i < num_samples]\n",
    "\n",
    "        # Exclude test and validation indices from training set\n",
    "        train_indices = [i for i in all_indices if i not in val_indices and i != test_index]\n",
    "\n",
    "        # Create datasets\n",
    "        dataset = TensorDataset(seismic_data, acoustic_impedance_data)\n",
    "\n",
    "        train_dataset = Subset(dataset, train_indices)\n",
    "        val_dataset = Subset(dataset, val_indices)\n",
    "\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "        unlabeled_loader = DataLoader(\n",
    "            TensorDataset(seismic_data[train_indices]),\n",
    "            batch_size=batch_size,\n",
    "            shuffle=True\n",
    "        )\n",
    "\n",
    "        return train_loader, val_loader, unlabeled_loader\n",
    "\n",
    "    else:\n",
    "        # Test mode\n",
    "        test_seismic = seismic_data[test_index].unsqueeze(0)\n",
    "        test_impedance = acoustic_impedance_data[test_index].unsqueeze(0)\n",
    "\n",
    "        test_dataset = TensorDataset(test_seismic, test_impedance)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "        return test_loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca2c39cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1).float()\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(1, 2)  # shape: (1, d_model, max_len)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.pe.size(2) < x.size(2):\n",
    "            # Upsample PE if input is longer than expected\n",
    "            pe = nn.functional.interpolate(self.pe, size=x.size(2), mode=\"linear\", align_corners=False)\n",
    "        else:\n",
    "            pe = self.pe[:, :, :x.size(2)]\n",
    "        return x + pe\n",
    "\n",
    "class DepthwiseSeparableConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, padding=1):\n",
    "        super().__init__()\n",
    "        self.depthwise = nn.Conv1d(in_channels, in_channels, kernel_size, padding=padding, groups=in_channels)\n",
    "        self.pointwise = nn.Conv1d(in_channels, out_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.pointwise(self.depthwise(x))\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            DepthwiseSeparableConv(in_channels, out_channels),\n",
    "            nn.GroupNorm(1, out_channels),\n",
    "            nn.ReLU(),\n",
    "            DepthwiseSeparableConv(out_channels, out_channels),\n",
    "            nn.GroupNorm(1, out_channels)\n",
    "        )\n",
    "        self.skip = nn.Conv1d(in_channels, out_channels, kernel_size=1) if in_channels != out_channels else nn.Identity()\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.relu(self.conv(x) + self.skip(x))\n",
    "\n",
    "\n",
    "class inverse_model(nn.Module):\n",
    "    def __init__(self, resolution_ratio=1, seq_len=940, nonlinearity=\"tanh\"):\n",
    "        super().__init__()\n",
    "        act_fn = nn.ReLU() if nonlinearity == \"relu\" else nn.Tanh()\n",
    "        self.activation = act_fn\n",
    "        self.seq_len = seq_len\n",
    "\n",
    "        # Inception-style convolution branches\n",
    "        self.inception = nn.ModuleList([\n",
    "            nn.Conv1d(1, 8, kernel_size=3, padding=1),\n",
    "            nn.Conv1d(1, 8, kernel_size=5, padding=2),\n",
    "            nn.Conv1d(1, 8, kernel_size=7, padding=3),\n",
    "        ])\n",
    "\n",
    "        # Fuse all branches into 32 channels\n",
    "        self.fuse = nn.Conv1d(24, 32, kernel_size=1)\n",
    "\n",
    "        # Encoder: Residual convolution blocks\n",
    "        self.encoder = nn.Sequential(\n",
    "            ResidualBlock(32, 64),\n",
    "            ResidualBlock(64, 64)\n",
    "        )\n",
    "\n",
    "        # Positional encoding + transformer\n",
    "        self.pos_enc = PositionalEncoding(64, max_len=seq_len)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=64, nhead=4, dim_feedforward=128, batch_first=True)\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=2)\n",
    "\n",
    "        # Decoder-like layers (keep same length, no upsampling)\n",
    "        self.decoder = nn.Sequential(\n",
    "            ResidualBlock(64, 32),\n",
    "            ResidualBlock(32, 16)\n",
    "        )\n",
    "\n",
    "        # Output layer: 1 channel\n",
    "        self.output_layer = nn.Conv1d(16, 1, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Inception convs\n",
    "        inc_out = torch.cat([conv(x) for conv in self.inception], dim=1)\n",
    "        x = self.fuse(inc_out)\n",
    "\n",
    "        # Residual encoder\n",
    "        x = self.encoder(x)\n",
    "\n",
    "        # Positional encoding + transformer\n",
    "        x = self.pos_enc(x)\n",
    "        x = x.transpose(1, 2)  # (B, L, C) for transformer\n",
    "        x = self.transformer(x)\n",
    "        x = x.transpose(1, 2)  # back to (B, C, L)\n",
    "\n",
    "        # Residual decoder (no upsampling)\n",
    "        x = self.decoder(x)\n",
    "\n",
    "        # Ensure consistent output length (940)\n",
    "        if x.size(-1) != self.seq_len:\n",
    "            x = nn.functional.interpolate(x, size=self.seq_len, mode=\"linear\", align_corners=False)\n",
    "\n",
    "        return self.output_layer(x)\n",
    "\n",
    "\n",
    "class forward_model(nn.Module):\n",
    "    def __init__(self, resolution_ratio=1, nonlinearity=\"tanh\"):\n",
    "        super().__init__()\n",
    "        act_fn = nn.ReLU() if nonlinearity == \"relu\" else nn.Tanh()\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "            ResidualBlock(1, 16),\n",
    "            ResidualBlock(16, 32),\n",
    "            ResidualBlock(32, 64),\n",
    "        )\n",
    "        self.wavelet = nn.Conv1d(64, 64, kernel_size=50, stride=resolution_ratio, padding=25)\n",
    "        self.output_layer = nn.Conv1d(64, 1, kernel_size=2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.wavelet(x)\n",
    "        return self.output_layer(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "763d2c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn, optim\n",
    "\n",
    "def get_models(args):\n",
    "\n",
    "    if args.test_checkpoint is None:\n",
    "        inverse_net = inverse_model(nonlinearity=args.nonlinearity)\n",
    "        forward_net = forward_model(nonlinearity=args.nonlinearity)\n",
    "        optimizer = optim.Adam(list(inverse_net.parameters())+list(forward_net.parameters()), amsgrad=True,lr=0.0005)\n",
    "    else:\n",
    "        try:\n",
    "        \n",
    "            inverse_net = torch.load(args.test_checkpoint + \"_inverse\", weights_only=False)\n",
    "\n",
    "            forward_net = torch.load(args.test_checkpoint + \"_forward\", weights_only=False)\n",
    "            optimizer = torch.load(args.test_checkpoint + \"_optimizer\", weights_only=False)\n",
    "\n",
    "        except FileNotFoundError:\n",
    "            print(\"No checkpoint found at '{}'- Please specify the model for testing\".format(args.test_checkpoint))\n",
    "            exit()\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        inverse_net.cuda()\n",
    "        forward_net.cuda()\n",
    "\n",
    "    return inverse_net, forward_net, optimizer\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a68065d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_1(args, seismic_data, acoustic_impedance_data, \n",
    "               seismic_normalization, acoustic_normalization,\n",
    "               test_index=0, test=False):\n",
    "    \"\"\"\n",
    "    Wrapper to prepare data loaders with normalization and test/train/validation split.\n",
    "\n",
    "    Args:\n",
    "        args: Argument object with batch_size\n",
    "        seismic_data (torch.Tensor): Normalized seismic tensor [12, 1, 940]\n",
    "        acoustic_impedance_data (torch.Tensor): Normalized impedance tensor [12, 1, 940]\n",
    "        seismic_normalization: Normalization info for seismic\n",
    "        acoustic_normalization: Normalization info for impedance\n",
    "        test_index (int): Index of test trace\n",
    "        test (bool): Whether to prepare test loader or training loaders\n",
    "\n",
    "    Returns:\n",
    "        If test:\n",
    "            test_loader, seismic_normalization, acoustic_normalization\n",
    "        Else:\n",
    "            train_loader, val_loader, unlabeled_loader, seismic_normalization, acoustic_normalization\n",
    "    \"\"\"\n",
    "    if test:\n",
    "        test_loader = prepare_data_loaders(\n",
    "            seismic_data,\n",
    "            acoustic_impedance_data,\n",
    "            test_index=test_index,\n",
    "            batch_size=args.batch_size,\n",
    "            test=True\n",
    "        )\n",
    "        return test_loader, seismic_normalization, acoustic_normalization\n",
    "\n",
    "    # Training mode with validation\n",
    "    train_loader, val_loader, unlabeled_loader = prepare_data_loaders(\n",
    "        seismic_data,\n",
    "        acoustic_impedance_data,\n",
    "        test_index=test_index,\n",
    "        batch_size=args.batch_size,\n",
    "        test=False\n",
    "    )\n",
    "\n",
    "    return train_loader, val_loader, unlabeled_loader, seismic_normalization, acoustic_normalization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2ed88ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from os.path import isdir\n",
    "\n",
    "\n",
    "# Example: converting your 12 trace arrays to tensors\n",
    "seismic_data = torch.tensor(np.array(seismic_data), dtype=torch.float32)  # shape [12, 1, 940]\n",
    "acoustic_impedance_data = torch.tensor(np.array(acoustic_impedance_data), dtype=torch.float32)  # shape [12, 1, 940]\n",
    "\n",
    "# Make sure these have shape [12, 1, ...] if they’re currently [12, ...]\n",
    "if seismic_data.ndim == 2:\n",
    "    #seismic_data = seismic_data\n",
    "    seismic_data = seismic_data.unsqueeze(1)\n",
    " \n",
    "if acoustic_impedance_data.ndim == 2:\n",
    "    #acoustic_impedance_data = acoustic_impedance_data\n",
    "    acoustic_impedance_data = acoustic_impedance_data.unsqueeze(1)\n",
    "\n",
    "\n",
    "\n",
    "def train(args, seismic_data, acoustic_impedance_data, \n",
    "          seismic_normalization, acoustic_normalization, \n",
    "          test_index=0):\n",
    "    \"\"\"\n",
    "    Train the inverse and forward models using all but one trace for training.\n",
    "\n",
    "    Args:\n",
    "        args: Argument object\n",
    "        seismic_data (torch.Tensor): [12, 1, 940]\n",
    "        acoustic_impedance_data (torch.Tensor): [12, 1, 940]\n",
    "        seismic_normalization: Normalization info\n",
    "        acoustic_normalization: Normalization info\n",
    "        test_index (int): Index of the trace to hold out for testing\n",
    "    \"\"\"\n",
    "    train_loader, val_loader, unlabeled_loader, _, _ = get_data_1(\n",
    "        args, seismic_data, acoustic_impedance_data,\n",
    "        seismic_normalization, acoustic_normalization,\n",
    "        test_index=test_index,\n",
    "        test=False\n",
    "    )\n",
    "\n",
    "    inverse_net, forward_net, optimizer = get_models(args)\n",
    "    inverse_net.train()\n",
    "    forward_net.train()\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    if not isdir(\"checkpoints\"):\n",
    "        os.mkdir(\"checkpoints\")\n",
    "\n",
    "    print(\"Training the model\")\n",
    "    best_loss = np.inf\n",
    "    train_losses, train_r2_scores = [], []\n",
    "    val_losses, val_r2_scores = [], []\n",
    "\n",
    "    for epoch in tqdm(range(args.max_epoch)):\n",
    "        train_loss = []\n",
    "        train_property_r2 = []\n",
    "\n",
    "        for x, y in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            y_pred = inverse_net(x)\n",
    "            x_rec = forward_net(y_pred)\n",
    "\n",
    "            property_loss = criterion(y_pred, y) + criterion(x_rec, x)\n",
    "            _, r2 = metrics(y_pred.detach(), y.detach())\n",
    "            train_property_r2.append(r2.mean().item())\n",
    "\n",
    "            seismic_loss = 0\n",
    "            if args.beta != 0:\n",
    "                try:\n",
    "                    x_u = next(unlabeled)[0]\n",
    "                except:\n",
    "                    unlabeled = iter(unlabeled_loader)\n",
    "                    x_u = next(unlabeled)[0]\n",
    "                y_u_pred = inverse_net(x_u)\n",
    "                x_u_rec = forward_net(y_u_pred)\n",
    "                seismic_loss = criterion(x_u_rec, x_u)\n",
    "\n",
    "            loss = args.alpha * property_loss + args.beta * seismic_loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss.append(loss.item())\n",
    "\n",
    "        mean_train_loss = np.mean(train_loss)\n",
    "        mean_train_r2 = np.mean(train_property_r2)\n",
    "        train_losses.append(mean_train_loss)\n",
    "        train_r2_scores.append(mean_train_r2)\n",
    "\n",
    "        # Validation step\n",
    "        inverse_net.eval()\n",
    "        forward_net.eval()\n",
    "        val_loss = []\n",
    "        val_property_r2 = []\n",
    "        with torch.no_grad():\n",
    "            for x_val, y_val in val_loader:\n",
    "                y_val_pred = inverse_net(x_val)\n",
    "                x_val_rec = forward_net(y_val_pred)\n",
    "\n",
    "                val_total_loss = criterion(y_val_pred, y_val) + criterion(x_val_rec, x_val)\n",
    "                #_, val_r2 = metrics(y_val_pred, y_val)\n",
    "                _, val_r2 = metrics(y_val_pred.detach(), y_val.detach())\n",
    "                val_loss.append(val_total_loss.item())\n",
    "                #val_property_r2.append(val_r2)\n",
    "                val_property_r2.append(val_r2.mean().item())\n",
    "\n",
    "        inverse_net.train()\n",
    "        forward_net.train()\n",
    "\n",
    "        mean_val_loss = np.mean(val_loss)\n",
    "        mean_val_r2 = np.mean(val_property_r2)\n",
    "        val_losses.append(mean_val_loss)\n",
    "        val_r2_scores.append(mean_val_r2)\n",
    "\n",
    "        if (epoch + 1) % (args.max_epoch // 10) == 0 or epoch == 0:\n",
    "            print(f\"[{epoch + 1}/{args.max_epoch}] \"\n",
    "                  f\"Train Loss = {mean_train_loss:.4f}, Train R² = {mean_train_r2:.4f} | \"\n",
    "                  f\"Val Loss = {mean_val_loss:.4f}, Val R² = {mean_val_r2:.4f}\")\n",
    "\n",
    "    torch.save(inverse_net, f\"checkpoints/{args.session_name}_inverse\")\n",
    "    torch.save(forward_net, f\"checkpoints/{args.session_name}_forward\")\n",
    "    torch.save(optimizer, f\"checkpoints/{args.session_name}_optimizer\")\n",
    "\n",
    "    # Plot training and validation metrics\n",
    "    plt.figure(figsize=(12, 5))\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(train_losses, label=\"Train Loss\")\n",
    "    plt.plot(val_losses, label=\"Val Loss\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(\"Training vs Validation Loss\")\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(train_r2_scores, label=\"Train R²\")\n",
    "    plt.plot(val_r2_scores, label=\"Val R²\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"R² Score\")\n",
    "    plt.title(\"Training vs Validation R²\")\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d1eb999",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "def test(args, seismic_data=None, acoustic_impedance_data=None,\n",
    "         seismic_normalization=None, acoustic_normalization=None):\n",
    "    \"\"\"\n",
    "    Test the inverse and forward models using the held-out trace.\n",
    "    If seismic_data and acoustic_impedance_data are provided, they are used directly.\n",
    "    Otherwise, test_loader is obtained from get_data_1().\n",
    "    \"\"\"\n",
    "    if not os.path.exists(\"output_images\"):\n",
    "        os.makedirs(\"output_images\")\n",
    "    if not os.path.exists(\"output_data\"):\n",
    "        os.makedirs(\"output_data\")\n",
    "\n",
    "    # Load test data\n",
    "    if seismic_data is not None and acoustic_impedance_data is not None:\n",
    "        test_loader, _, _ = get_data_1(\n",
    "            args, seismic_data, acoustic_impedance_data,\n",
    "            seismic_normalization, acoustic_normalization,\n",
    "            test_index=args.test_index,\n",
    "            test=True\n",
    "        )\n",
    "    else:\n",
    "        test_loader, seismic_normalization, acoustic_normalization = get_data_1(args, test=True)\n",
    "\n",
    "    # Load models\n",
    "    if args.test_checkpoint is None:\n",
    "        args.test_checkpoint = f\"checkpoints/{args.session_name}\"\n",
    "\n",
    "    inverse_net, forward_net, _ = get_models(args)\n",
    "    inverse_net.eval()\n",
    "    forward_net.eval()\n",
    "\n",
    "    criterion = nn.MSELoss(reduction=\"mean\")\n",
    "    predicted_impedance = []\n",
    "    true_impedance = []\n",
    "    predicted_seismic = []\n",
    "    true_seismic = []\n",
    "    test_property_corr = []\n",
    "    test_property_r2 = []\n",
    "    test_seismic_corr=[]\n",
    "    test_seismic_r2=[]\n",
    "\n",
    "    print(\"\\nTesting the model...\\n\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        test_loss = []\n",
    "        for x, y in test_loader:\n",
    "            if torch.cuda.is_available():\n",
    "                x = x.cuda()\n",
    "                y = y.cuda()\n",
    "\n",
    "            y_pred = inverse_net(x)\n",
    "            property_loss = criterion(y_pred, y) / np.prod(y.shape)\n",
    "\n",
    "            corr, r2 = metrics(y_pred.detach(), y.detach())\n",
    "            test_property_corr.append(corr)\n",
    "            test_property_r2.append(r2)\n",
    "\n",
    "            x_rec = forward_net(y_pred)\n",
    "            corr_seismic, r2_seismic = metrics(x_rec.detach(), x.detach())\n",
    "            test_seismic_corr.append(corr_seismic)\n",
    "            test_seismic_r2.append(r2_seismic)\n",
    "            seismic_loss = criterion(x_rec, x) / np.prod(x.shape)\n",
    "\n",
    "            loss = args.alpha * property_loss + args.beta * seismic_loss\n",
    "            test_loss.append(loss.item())\n",
    "\n",
    "            true_impedance.append(y.cpu())\n",
    "            predicted_impedance.append(y_pred.cpu())\n",
    "            true_seismic.append(x.cpu())\n",
    "            predicted_seismic.append(x_rec.cpu())\n",
    "\n",
    "    # Display metrics\n",
    "    display_results(test_loss, test_property_corr, test_property_r2, test_seismic_corr, test_seismic_r2, args, header=\"Test\")\n",
    "\n",
    "    # Concatenate and unnormalize\n",
    "    predicted_impedance = acoustic_normalization.unnormalize(torch.cat(predicted_impedance, dim=0)).numpy()\n",
    "    true_impedance = acoustic_normalization.unnormalize(torch.cat(true_impedance, dim=0)).numpy()\n",
    "    predicted_seismic = seismic_normalization.unnormalize(torch.cat(predicted_seismic, dim=0)).numpy()\n",
    "    true_seismic = seismic_normalization.unnormalize(torch.cat(true_seismic, dim=0)).numpy()\n",
    "\n",
    "    # Save outputs\n",
    "    np.save(f\"output_data/{args.session_name}_predicted_impedance.npy\", predicted_impedance)\n",
    "    np.save(f\"output_data/{args.session_name}_true_impedance.npy\", true_impedance)\n",
    "    np.save(f\"output_data/{args.session_name}_predicted_seismic.npy\", predicted_seismic)\n",
    "    np.save(f\"output_data/{args.session_name}_true_seismic.npy\", true_seismic)\n",
    "\n",
    "    import matplotlib.gridspec as gridspec\n",
    "\n",
    "     # Create figure and grid layout\n",
    "    plt.figure(figsize=(14, 10))\n",
    "    gs = gridspec.GridSpec(3, 2, width_ratios=[20, 1])  # One wide column for image, one narrow for colorbar\n",
    "  \n",
    "        # Select one test trace to plot (e.g., the first trace)\n",
    "    predicted_trace = predicted_impedance[0].squeeze()  # shape: (240,)\n",
    "    true_trace = true_impedance[0].squeeze()            # shape: (240,)\n",
    "    #predicted_trace=predicted_impedance\n",
    "    #true_trace= true_impedance\n",
    "\n",
    "     # Estimated Impedance\n",
    "        # Estimated Impedance (1D Trace)\n",
    "    ax0 = plt.subplot(gs[0, 0])\n",
    "    ax0.plot(predicted_trace, label=\"Predicted\", color='red', linestyle='--')\n",
    "    ax0.plot(true_trace, label=\"True\", color='blue')\n",
    "    ax0.set_title(\"Estimated vs True Acoustic Impedance (Trace)\")\n",
    "    ax0.set_xlabel(\"Time/depth sample index\")\n",
    "    ax0.set_ylabel(\"Impedance\")\n",
    "    ax0.legend()\n",
    "    ax0.grid(True)\n",
    "\n",
    "###############################################################################\n",
    "      # True Impedance\n",
    "        # ax1 = plt.subplot(gs[1, 0])\n",
    "        # ax1.plot(true_impedance, label=\"True\", color='blue')\n",
    "        # ax1.plot(predicted_impedance, label=\"Predicted\", color='red', linestyle='--')\n",
    "        # ax1.set_title(\"True vs Predicted Impedance (Test Trace)\")\n",
    "        # ax1.set_xlabel(\"Time/depth sample index\")\n",
    "        # ax1.set_ylabel(\"Impedance\")\n",
    "        # ax1.legend()\n",
    "        # # im1 = ax1.imshow(true_impedance[:, 0].T, cmap='rainbow', aspect=0.5,\n",
    "        # #          vmin=true_impedance.min(), vmax=true_impedance.max())\n",
    "        # ax1.axis('off')\n",
    "        # ax1.set_title(\"True Acoustic Impedance\")\n",
    "       # plt.colorbar(im1, cax=plt.subplot(gs[1, 1]))\n",
    "#######################################################################################\n",
    "       # Absolute Difference\n",
    "        # Absolute Difference (1D Trace)\n",
    "    ax2 = plt.subplot(gs[2, 0])\n",
    "    abs_diff = np.abs(true_trace - predicted_trace)\n",
    "    ax2.plot(abs_diff, color='black')\n",
    "    ax2.set_title(\"Absolute Difference (Trace)\")\n",
    "    ax2.set_xlabel(\"Time/depth sample index\")\n",
    "    ax2.set_ylabel(\"Abs Difference\")\n",
    "    ax2.grid(True)\n",
    "\n",
    "        #plt.colorbar(im2, cax=plt.subplot(gs[2, 1]))\n",
    "\n",
    "    plt.tight_layout()\n",
    "    out_file = f\"output_images/{args.test_checkpoint.split('/')[-1]}.png\"\n",
    "    plt.savefig(out_file,dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "        \n",
    "    # import matplotlib.gridspec as gridspec\n",
    "        \n",
    "\n",
    "    plt.figure(figsize=(14, 10))\n",
    "    gs = gridspec.GridSpec(3, 2, width_ratios=[20, 1])  # One wide column for image, one narrow for colorbar\n",
    "  \n",
    "        # Select one test trace to plot (e.g., the first trace)\n",
    "    predicted__seismic_trace = predicted_seismic[0].squeeze()  # shape: (240,)\n",
    "    true_seismic_trace = true_seismic[0].squeeze()            # shape: (240,)\n",
    "    #predicted__seismic_trace = predicted_seismic\n",
    "    #true_seismic_trace = true_seismic\n",
    "\n",
    "     # Estimated Impedance\n",
    "        # Estimated Impedance (1D Trace)\n",
    "    ax0 = plt.subplot(gs[0, 0])\n",
    "    ax0.plot(predicted__seismic_trace, label=\"Predicted\", color='red', linestyle='--')\n",
    "    ax0.plot(true_seismic_trace, label=\"True\", color='blue')\n",
    "    ax0.set_title(\"Estimated vs True Acoustic seismic (Trace)\")\n",
    "    ax0.set_xlabel(\"Time/depth sample index\")\n",
    "    ax0.set_ylabel(\"seismic\")\n",
    "    ax0.legend()\n",
    "    ax0.grid(True)\n",
    "\n",
    "    # # Plot: True Seismic\n",
    "    # ax1 = fig.add_subplot(gs[1, 0])\n",
    "    # im1 = ax1.imshow(true_seismic[:, 0].T, cmap='gray', aspect='auto',\n",
    "    #          vmin=true_seismic.min(), vmax=true_seismic.max())\n",
    "    # ax1.set_title(\"True Seismic\", fontsize=12)\n",
    "    # ax1.axis('off')\n",
    "    # cbar1 = fig.add_subplot(gs[1, 1])\n",
    "    # plt.colorbar(im1, cax=cbar1)\n",
    "\n",
    "    # # Plot: Absolute Difference\n",
    "    ax2 = plt.subplot(gs[2, 0])\n",
    "    abs_diff = np.abs(true_seismic_trace - predicted__seismic_trace)\n",
    "    ax2.plot(abs_diff, color='black')\n",
    "    ax2.set_title(\"Absolute Difference (Trace)\")\n",
    "    ax2.set_xlabel(\"Time/depth sample index\")\n",
    "    ax2.set_ylabel(\"Abs Difference\")\n",
    "    ax2.grid(True)\n",
    "\n",
    "        #plt.colorbar(im2, cax=plt.subplot(gs[2, 1]))\n",
    "\n",
    "    plt.tight_layout()\n",
    "    out_file = f\"output_images/{args.test_checkpoint.split('/')[-1]}.png\"\n",
    "    plt.savefig(out_file,dpi=300, bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fadbb52",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    import argparse\n",
    "    from datetime import datetime\n",
    "    import sys\n",
    "\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('-max_epoch', type=int, default=10)\n",
    "    parser.add_argument('-batch_size', type=int, default=20)\n",
    "    parser.add_argument('-alpha', type=float, default=10)\n",
    "    parser.add_argument('-beta', type=float, default=5)\n",
    "    parser.add_argument('-test_checkpoint', type=str, default=None)\n",
    "    parser.add_argument('-session_name', type=str, default=datetime.now().strftime('%b%d_%H%M%S'))\n",
    "    parser.add_argument('-nonlinearity', type=str, default=\"tanh\", choices=[\"tanh\", \"relu\"])\n",
    "    parser.add_argument('-resolution_ratio', type=int, default=1)\n",
    "    parser.add_argument('-seed', type=int, default=42)\n",
    "    parser.add_argument('-test_index', type=int, default=0)  # <- Let user choose the blind trace\n",
    "\n",
    "    args, unknown = parser.parse_known_args(sys.argv[1:])\n",
    "\n",
    "    # ==== Load and normalize data ====\n",
    "    # (Replace with your actual loading and normalization logic)\n",
    "    # seismic_data, acoustic_impedance_data = ...\n",
    "    # seismic_normalization, acoustic_normalization = ...\n",
    "\n",
    "    # ==== Execute mode ====\n",
    "    # Then pass it into train/test based on mode\n",
    "    if args.test_checkpoint is not None:\n",
    "        test(args)\n",
    "    else:\n",
    "        train(args, seismic_data, acoustic_impedance_data,\n",
    "              seismic_normalization, acoustic_normalization,\n",
    "              test_index=args.test_index)\n",
    "        test(args, seismic_data, acoustic_impedance_data,\n",
    "              seismic_normalization, acoustic_normalization)\n",
    "              #test_index=args.test_index)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
